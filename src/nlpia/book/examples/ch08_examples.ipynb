{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e5b253-49da-49fa-bdfe-84f54928dfac",
   "metadata": {},
   "source": [
    "# 8. Loopy (recurrent) neural networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8a823-b9de-4b26-913e-4a68468bd81e",
   "metadata": {},
   "source": [
    "### 8.1.5 Recurrent neural net with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d1d475b-df67-4ae1-89af-093e6542a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../../bigdata/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9870e479-604f-4bf6-b5f2-43a0c670394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "\n",
    "    shuffle(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8667254-53ba-44a2-b859-5c54f67ccae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4123a82c-91f3-43da-ba4c-3ae161d85765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb7ace27-4de5-439e-bfd5-56d547a1d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "dataset = pre_process_data('../../bigdata/aclImdb/train/')\n",
    "\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "split_point = int(len(vectorized_data) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83618dcc-3e67-4f74-8702-ca2e634c26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "# filters = 250           # Number of filters we will train\n",
    "# kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "# hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "088bc399-7c45-4e38-bc72-7fb6c2a44a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af20eeb3-f982-40cb-ae72-a36ec31d45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "715ff5a2-b510-4da1-852b-42ea5ceff7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce test/train data by factor of 8\n",
      "Before changes: 20000 and 5000\n",
      "After reduction: 2500 and 625\n",
      "After pad_trunc: 2500 and 625\n",
      "400 300\n"
     ]
    }
   ],
   "source": [
    "## need to artificially reduce the size of test/train data for it to run on my machine\n",
    "## then do the pad_trunc as documented\n",
    "\n",
    "reducer = 8\n",
    "print(f\"Reduce test/train data by factor of {reducer}\")\n",
    "\n",
    "print(f\"Before changes: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = x_train[:int(len(x_train) / reducer)]\n",
    "x_test = x_test[:int(len(x_test) / reducer)]\n",
    "\n",
    "y_train = y_train[:int(len(y_train) / reducer)]\n",
    "y_test = y_test[:int(len(y_test) / reducer)]\n",
    "\n",
    "print(f\"After reduction: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "print(f\"After pad_trunc: {len(x_train)} and {len(x_test)}\")\n",
    "print(maxlen, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5503980-3662-4131-adca-6a10da01ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7e9f8cb-6412-4546-bca6-086ee9b488ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbbf7234-469c-469d-a1bf-f952650adaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 21:30:35.286829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 21:30:36.820801: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-21 21:30:36.820918: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-21 21:30:43.763862: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 21:30:43.764171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 21:30:43.764218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 21:30:47.639946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-21 21:30:47.641151: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-21 21:30:47.641273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (04896aa7ca98): /proc/driver/nvidia/version does not exist\n",
      "2022-11-21 21:30:47.643810: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5841903a-35f2-46b8-8b20-432b2c6f13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265ca783-ebfb-430b-b85b-9b80f9cf6c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2eb9b0-4900-4fa6-a10a-e446bd886bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a2a015-e253-440e-a3a0-fbf7b0b854a0",
   "metadata": {},
   "source": [
    "## 8.2 Putting things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad998f4-0909-4bbf-a178-ee6d5c5ca341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 400, 50)           17550     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400, 50)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 20001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,551\n",
      "Trainable params: 37,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba299617-627f-46ff-92c7-5bd5be304b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8070821b-32b8-4fbf-9bf2-41c2c64c6b16",
   "metadata": {},
   "source": [
    "## 8.3 Let's get to learning our past selves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e88ec49-781d-4c13-a7a5-23a882594baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 21:31:11.650668: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "79/79 [==============================] - ETA: 0s - loss: 0.7258 - accuracy: 0.5748"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 21:31:48.429561: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 300000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 16s 170ms/step - loss: 0.7258 - accuracy: 0.5748 - val_loss: 0.5925 - val_accuracy: 0.6864\n",
      "Epoch 2/2\n",
      "79/79 [==============================] - 11s 136ms/step - loss: 0.4801 - accuracy: 0.7664 - val_loss: 0.6037 - val_accuracy: 0.6704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7cc8445900>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b71a5ca-c6c3-48a7-830c-66839a323536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "    \n",
    "model.save_weights(\"simplernn_weights1.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd6df0-fade-41e5-ae42-aa3e111dda6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f1b7beb-325b-4572-a887-8d7bfd84efa8",
   "metadata": {},
   "source": [
    "## 8.4 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c4da85-5469-43a0-9a7a-988fb1d29d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (None, 400, 100)          40100     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 400, 100)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 40000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 40001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,101\n",
      "Trainable params: 80,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "num_neurons = 100\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c76b914-9b59-4f5f-aad1-d3b0dee6a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting to big for my laptop :(\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# model_structure = model.to_json()\n",
    "# with open(\"simplernn_model2.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_structure)\n",
    "\n",
    "# model.save_weights(\"simplernn_weights2.h5\")\n",
    "# print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37056745-7a5a-4154-8208-575803f6b6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8790fe80-4f23-4462-bbc5-fbd550f405e6",
   "metadata": {},
   "source": [
    "## 8.5 Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7406faca-362e-4bdf-b2f3-53c07e983b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"I hate that the dismal weather that had me down for so long, when will it break!\" \\\n",
    "    \"Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin. \" \\\n",
    "    \"I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8467d053-4f2d-4b1f-a8a3-168ab55ac7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"simplernn_model1.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77467113-ed35-4e5e-9bf9-71081f2216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('simplernn_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69d80ca3-5d03-4350-a69c-ab66608a1da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62a89106-0be5-4230-a0f2-18d5f4bba22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e85d24f3-57fb-497b-b481-6d1548ad0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a00fec9-b0d9-45e1-bdb1-dfc350abc118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 231ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.24704814]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3918b686-390c-4e0f-a666-a55b1cf1bf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a278ac-6ee9-489d-8936-4cf6a7c9d830",
   "metadata": {},
   "source": [
    "### 8.5.2 Two-way street"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3cf6818-6606-43fa-ba80-6ecfa2041a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "num_neurons = 10\n",
    "maxlen = 100\n",
    "embedding_dims = 300\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(\n",
    "    num_neurons, return_sequences=True), \\\n",
    "    input_shape=(maxlen, embedding_dims)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
