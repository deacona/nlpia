{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e5b253-49da-49fa-bdfe-84f54928dfac",
   "metadata": {},
   "source": [
    "# 13. Scaling up (optimization, parallelization, and batch processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8a823-b9de-4b26-913e-4a68468bd81e",
   "metadata": {},
   "source": [
    "###  13.2.3 Advanced indxing with Annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dadb1785-5727-44ba-9412-2fd522be1f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "# from nlpia.data.loaders import BIGDATA_PATH\n",
    "\n",
    "# not in book, reader required to compose this path\n",
    "# wordvector_path = os.path.join(BIGDATA_PATH, 'GoogleNews-vectors-negative300.bin.gz')\n",
    "## https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g\n",
    "wordvector_path = os.path.join(\"../../bigdata\", 'GoogleNews-vectors-negative300.bin.gz')\n",
    "\n",
    "wv = KeyedVectors.load_word2vec_format(wordvector_path, binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "129a5d78-4e07-492d-92f0-cd6319614ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wv.key_to_index), len(wv[next(iter(wv.key_to_index))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "febc0d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ac2a5c3-0c6f-4ad6-98df-4be97b7ad9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "  Downloading annoy-1.17.1.tar.gz (647 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m648.0/648.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: annoy\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for annoy: filename=annoy-1.17.1-cp310-cp310-linux_x86_64.whl size=76888 sha256=e67eb26179f1ce8b09f0e5866f9c9a55b9ce0936d1e7515ece6fa0794ad2b57e\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/8a/8b/ca301ec85de2c145c45b09994765966c7148e54dbbf2b8bfff\n",
      "Successfully built annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --user annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68ad887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4370/487472667.py:3: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  index = AnnoyIndex(num_dimensions)\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "num_words, num_dimensions = wv.vectors.shape  # <1>\n",
    "index = AnnoyIndex(num_dimensions)\n",
    "index.set_seed(1983)\n",
    "\n",
    "# <1> The original GoogleNews word2vec model contains 3M word vectors, each with 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4917165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200000/200000 [00:04<00:00, 42120.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, word in enumerate(tqdm(wv.index_to_key)):  # <1> & <2>\n",
    "    index.add_item(i, wv[word])\n",
    "\n",
    "# <1> `tqdm()` takes an iterable and returns an iterable (like `enumerate()`) and inserts code in your loop to display a progress bar\n",
    "# <2> `.index2word` is an unsorted list of all 3M tokens in your vocabulary, equivalent to a map of the integer indexes (0-2999999) to \n",
    "#     tokens ('</s>' to 'snowcapped_Caucasus')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71bacd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_trees = int(np.log(num_words).round(0))  # <1>\n",
    "print(num_trees)\n",
    "\n",
    "index.build(num_trees)  # <2>\n",
    "index.save('Word2vec_euc_index.ann')  # <3>\n",
    "\n",
    "# <1> This is just a rule of thumb -- you may want to optimize this hyperparameter if this index isn't performant for the things you care about (RAM, lookup, indexing) or accurate enough for your application.\n",
    "# <2> round(ln(3000000)) => 15 indexing trees for our 3M vectors -- this takes a few minutes on a laptop\n",
    "# <3> Saves the index to a local file and frees up RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a56378e4-18fb-4812-b237-d161a0614dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9494"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.key_to_index['Harry_Potter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8c43739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9494"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wv.vocab['Harry_Potter'].index  # <1>\n",
    "wv.key_to_index['Harry_Potter']  # <1>\n",
    "# 9494\n",
    "# <1> The gensim KeyedVectors.vocab dict contains Vocab objects rather than raw strings or index numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "954bc21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190506"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wv.vocab['Harry_Potter'].count  # <2>\n",
    "wv.get_vecattr(\"Harry_Potter\", \"count\")  # <2>\n",
    "# 2990506\n",
    "# <2> The gensim Vocab object can tell you the number of times the \"Harry_Potter\" 2-gram was mentioned in the googleNews corpus... nearly 3M times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea43ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2id = dict(zip(\n",
    "#     wv.vocab, range(len(wv.vocab))))  # <3>\n",
    "w2id = wv.key_to_index\n",
    "# <3> Create a map similar to mv,vocab, mapping the tokens to their index values (integer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12eb9360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9494"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2id['Harry_Potter']\n",
    "# 9494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9446e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9494, 39034, 114813, 172698, 59576, 15107, 145465, 15396, 58514, 22364, 22105]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = index.get_nns_by_item(\n",
    "    w2id['Harry_Potter'], 11)  # <4>\n",
    "ids\n",
    "\n",
    "# <4> Annoy returns the target vector first, so we have to request 11 \"neighbours\" if we want t10 in addition to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86e348d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [wv.vocab[i] for i in _]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e7249e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry_Potter',\n",
       " 'Sherlock_Holmes',\n",
       " 'Lemony_Snicket',\n",
       " 'Spiderwick_Chronicles',\n",
       " 'Superman_Returns',\n",
       " 'comic_book',\n",
       " 'Unfortunate_Events',\n",
       " 'Batman',\n",
       " 'Goblet',\n",
       " 'Shrek',\n",
       " 'Transformers']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [wv.index2word[i] for i in ids]\n",
    "[wv.index_to_key[i] for i in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b43ffb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JK_Rowling_Harry_Potter',\n",
       " 'JK_Rowling',\n",
       " 'boy_wizard',\n",
       " 'Deathly_Hallows',\n",
       " 'Half_Blood_Prince',\n",
       " 'Rowling',\n",
       " 'Twilight',\n",
       " 'Twilight_saga',\n",
       " 'author_JK_Rowling',\n",
       " 'Narnia']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word, similarity in wv.most_similar('Harry_Potter', topn=10)]\n",
    "# ['JK_Rowling_Harry_Potter',\n",
    "#  'JK_Rowling',\n",
    "#  'boy_wizard',\n",
    "#  'Deathly_Hallows',\n",
    "#  'Half_Blood_Prince',\n",
    "#  'Rowling',\n",
    "#  'Actor_Rupert_Grint',\n",
    "#  'HARRY_Potter',\n",
    "#  'wizard_Harry_Potter',\n",
    "#  'HARRY_POTTER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f349ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: </s>\n",
      "100000: distinctiveness\n"
     ]
    }
   ],
   "source": [
    "index_cos = AnnoyIndex(\n",
    "    f=num_dimensions, metric=\"angular\" # <1>\n",
    ")\n",
    "# for i, word in enumerate(wv.index2word):\n",
    "for i, word in enumerate(wv.index_to_key):\n",
    "    if not i % 100000:\n",
    "        print('{}: {}'.format(i, word)) # <2>\n",
    "    index_cos.add_item(i, wv[word])\n",
    "\n",
    "# <1> metric='angular' ues the angular (cosine) distance metric to compute your clusters and hashes.\n",
    "#     Your options are: 'angular', 'euclidean', 'manhattan', or 'hamming'.\n",
    "# <2> Another way to keep informed of your progress if you don't like tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbe2eee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_cos.build(30)  # <1>\n",
    "index_cos.save('Word2vec_cos_index.ann')\n",
    "\n",
    "# <1> 30 equals int(np.log(num_vectors).round(0)), double what you had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91f15fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9494, 193309, 37681, 71557, 40544, 41526, 30024, 78932, 32643, 84628]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_cos = index_cos.get_nns_by_item(w2id['Harry_Potter'], 10)\n",
    "idx_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7089456d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry_Potter',\n",
       " 'JK_Rowling_Harry_Potter',\n",
       " 'JK_Rowling',\n",
       " 'boy_wizard',\n",
       " 'Deathly_Hallows',\n",
       " 'Half_Blood_Prince',\n",
       " 'Rowling',\n",
       " 'author_JK_Rowling',\n",
       " 'Narnia',\n",
       " 'Stephenie_Meyer']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [wv.index2word[i] for i in ids_cos] # <1>\n",
    "[wv.index_to_key[i] for i in idx_cos] # <1>\n",
    "\n",
    "# <1> You'll not get the same results. Random projection for LSH is stochastic. Use AnnoyIndex.set_seed() if you need repeatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cc77755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annoy_15trees</th>\n",
       "      <th>annoy_30trees</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gensim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>JK_Rowling_Harry_Potter</th>\n",
       "      <td>Harry_Potter</td>\n",
       "      <td>Harry_Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JK_Rowling</th>\n",
       "      <td>Sherlock_Holmes</td>\n",
       "      <td>JK_Rowling_Harry_Potter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boy_wizard</th>\n",
       "      <td>Lemony_Snicket</td>\n",
       "      <td>JK_Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deathly_Hallows</th>\n",
       "      <td>Spiderwick_Chronicles</td>\n",
       "      <td>boy_wizard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Half_Blood_Prince</th>\n",
       "      <td>Superman_Returns</td>\n",
       "      <td>Deathly_Hallows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rowling</th>\n",
       "      <td>comic_book</td>\n",
       "      <td>Half_Blood_Prince</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twilight</th>\n",
       "      <td>Unfortunate_Events</td>\n",
       "      <td>Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Twilight_saga</th>\n",
       "      <td>Batman</td>\n",
       "      <td>author_JK_Rowling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_JK_Rowling</th>\n",
       "      <td>Goblet</td>\n",
       "      <td>Narnia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narnia</th>\n",
       "      <td>Shrek</td>\n",
       "      <td>Stephenie_Meyer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 annoy_15trees            annoy_30trees\n",
       "gensim                                                                 \n",
       "JK_Rowling_Harry_Potter           Harry_Potter             Harry_Potter\n",
       "JK_Rowling                     Sherlock_Holmes  JK_Rowling_Harry_Potter\n",
       "boy_wizard                      Lemony_Snicket               JK_Rowling\n",
       "Deathly_Hallows          Spiderwick_Chronicles               boy_wizard\n",
       "Half_Blood_Prince             Superman_Returns          Deathly_Hallows\n",
       "Rowling                             comic_book        Half_Blood_Prince\n",
       "Twilight                    Unfortunate_Events                  Rowling\n",
       "Twilight_saga                           Batman        author_JK_Rowling\n",
       "author_JK_Rowling                       Goblet                   Narnia\n",
       "Narnia                                   Shrek          Stephenie_Meyer"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gensim_top10 = pd.Series([word for word, similarity in wv.most_similar('Harry_Potter', topn=10)])\n",
    "\n",
    "annoy_top10_15 = pd.Series([wv.index_to_key[i] for i in ids])\n",
    "\n",
    "annoy_top10_30 = pd.Series([wv.index_to_key[i] for i in idx_cos])\n",
    "\n",
    "top10s = pd.concat([gensim_top10, annoy_top10_15, annoy_top10_30], axis=1).head(10)\n",
    "top10s.columns = [\"gensim\", \"annoy_15trees\", \"annoy_30trees\"]\n",
    "top10s.set_index(\"gensim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625fd761",
   "metadata": {},
   "source": [
    "### 13.2.5 An indexing workaround: discretizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3ce71ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.2, 3.4, 5.6, -7.8, 9.0]\n",
      "[-1.2  3.4  5.6 -7.8  9. ]\n",
      "[[-1.2]\n",
      " [ 3.4]\n",
      " [ 5.6]\n",
      " [-7.8]\n",
      " [ 9. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[39, 66, 79, 0, 100]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "real_values = [-1.2, 3.4, 5.6, -7.8, 9.0]\n",
    "print(real_values)\n",
    "real_values = np.array(real_values)\n",
    "print(real_values)\n",
    "real_values = real_values.reshape(-1, 1)\n",
    "print(real_values)\n",
    "\n",
    "scaler = MinMaxScaler()  # <1>\n",
    "scaler.fit(real_values)\n",
    "[int(x * 100.) for x in scaler.transform(real_values)]  # <2>\n",
    "\n",
    "# <1> Confine our floats to be between 0.0 and 1.0.\n",
    "# <2> Scaled, discretized ints, 0-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b23535",
   "metadata": {},
   "source": [
    "### 13.6.1 How to visualize word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35777725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (2.11.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.19.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.38.4)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.23.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (65.5.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.14.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (1.50.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5e3659f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-03 14:10:35.796549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-03 14:10:36.850661: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-03 14:10:36.850730: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-03 14:10:39.598231: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-03 14:10:39.598467: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-03 14:10:39.598498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-01-03 14:10:42.550086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-03 14:10:42.551669: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-03 14:10:42.551770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aaf1185f7de8): /proc/driver/nvidia/version does not exist\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.11.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=/tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c6a53239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from io import open\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "def create_projection(projection_data,\n",
    "                     projection_name=\"tensorboard_viz\",\n",
    "                     path=\"/tmp/\"):  # <1>\n",
    "    meta_file = \"{}.tsv\".format(projection_name)\n",
    "    vector_dim = len(projection_data[0][1])\n",
    "    samples = len(projection_data)\n",
    "    projection_matrix = np.zeros((samples, vector_dim))\n",
    "\n",
    "    with open(os.path.join(path, meta_file), \"w\") as file_metadata:\n",
    "        for i, row in enumerate(projection_data):  # <2>\n",
    "            label, vector = row[0], row[1]\n",
    "            projection_matrix[i] = np.array(vector)\n",
    "            file_metadata.write(\"{}\\n\".format(label))\n",
    "            \n",
    "    print(projection_matrix)\n",
    "\n",
    "    # sess = tf.InteractiveSession()  # <3>\n",
    "    sess = tf.compat.v1.InteractiveSession()\n",
    "\n",
    "    embedding = tf.Variable(projection_matrix,\n",
    "                            trainable=False,\n",
    "                            name=projection_name)\n",
    "    # tf.global_variables_initializer().run()\n",
    "    # tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "    # saver = tf.train.Saver()\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    # writer = tf.summary.FileWriter(path, sess.graph)  # <4>\n",
    "    # writer = tf.compat.v1.summary.FileWriter(path, sess.graph)  # <4>\n",
    "    # writer = tf.summary.create_file_writer(path, sess.graph)  # <4>\n",
    "    with tf.compat.v1.Graph().as_default():\n",
    "        writer = tf.compat.v1.summary.FileWriter(path, sess.graph)  # <4>\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "\n",
    "    embed.tensor_name = \"{}\".format(projection_name)\n",
    "    embed.metadata_path = os.path.join(path, meta_file)\n",
    "\n",
    "    projector.visualize_embeddings(writer, config)  # <5>\n",
    "    # print(sess)\n",
    "    saver.save(sess, os.path.join(path, \"{}.ckpt\".format(projection_name)))    \n",
    "    sess.close()\n",
    "\n",
    "    print(\"Run `tensorboard --logdir={0}` to run\\\n",
    "           visualize result on tensorboard\".format(path))\n",
    "\n",
    "# <1> The create_projection function takes three arguments: the embedding data, a name for the projection and a path, and where to\n",
    "#     store the projection files.\n",
    "# <2> The function loops over the embedding data and creates a numpy array, which will then be converted to a Tensorflow variable.\n",
    "# <3> To create the TensorBoard projection, you need to create a Tensorflow session.\n",
    "# <4> TensorFlow provides built-in methods to create projections.\n",
    "# <5> visualize_embeddings writes the projection to your path and is then available for TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ce85b82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34 -0.72]\n",
      " [ 0.46  0.39]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node 'NLP_in_Action/Read/ReadVariableOp' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4370/3755568974.py\", line 7, in <module>\n      create_projection(projection_data, projection_name)\n    File \"/tmp/ipykernel_4370/1444917779.py\", line 28, in create_projection\n      embedding = tf.Variable(projection_matrix,\nNode: 'NLP_in_Action/Read/ReadVariableOp'\nCould not find variable NLP_in_Action. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/NLP_in_Action)\n\t [[{{node NLP_in_Action/Read/ReadVariableOp}}]]\n\nOriginal stack trace for 'NLP_in_Action/Read/ReadVariableOp':\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_4370/3755568974.py\", line 7, in <module>\n    create_projection(projection_data, projection_name)\n  File \"/tmp/ipykernel_4370/1444917779.py\", line 28, in create_projection\n    embedding = tf.Variable(projection_matrix,\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 271, in __call__\n    return cls._variable_v2_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 250, in _variable_v2_call\n    return previous_getter(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 243, in <lambda>\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variable_scope.py\", line 2758, in default_variable_creator_v2\n    return resource_variable_ops.ResourceVariable(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 273, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1721, in __init__\n    self._init_from_args(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1963, in _init_from_args\n    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 539, in read_variable_op\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Could not find variable NLP_in_Action. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/NLP_in_Action)\n\t [[{{node NLP_in_Action/Read/ReadVariableOp}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [108], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m projection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNLP_in_Action\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m projection_data \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.34\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.72\u001b[39m]),\n\u001b[1;32m      4\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoy\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.46\u001b[39m, \u001b[38;5;241m0.39\u001b[39m])\n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mcreate_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojection_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [107], line 50\u001b[0m, in \u001b[0;36mcreate_projection\u001b[0;34m(projection_data, projection_name, path)\u001b[0m\n\u001b[1;32m     48\u001b[0m projector\u001b[38;5;241m.\u001b[39mvisualize_embeddings(writer, config)  \u001b[38;5;66;03m# <5>\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# print(sess)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43msaver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43msess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprojection_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun `tensorboard --logdir=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m` to run\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124m       visualize result on tensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/saver.py:1303\u001b[0m, in \u001b[0;36mSaver.save\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gfile\u001b[38;5;241m.\u001b[39mIsDirectory(save_path_parent):\n\u001b[1;32m   1300\u001b[0m       exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1301\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParent directory of \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt save.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1302\u001b[0m               save_path))\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1305\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   1306\u001b[0m metrics\u001b[38;5;241m.\u001b[39mAddCheckpointWriteDuration(\n\u001b[1;32m   1307\u001b[0m     api_label\u001b[38;5;241m=\u001b[39m_SAVER_LABEL,\n\u001b[1;32m   1308\u001b[0m     microseconds\u001b[38;5;241m=\u001b[39m_get_duration_microseconds(start_time, end_time))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/training/saver.py:1284\u001b[0m, in \u001b[0;36mSaver.save\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001b[0m\n\u001b[1;32m   1282\u001b[0m   model_checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaver_def\u001b[38;5;241m.\u001b[39msave_tensor_name\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m   model_checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaver_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_tensor_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m      \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaver_def\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_tensor_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m model_checkpoint_path \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_str(model_checkpoint_path)\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m write_state:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/client/session.py:1397\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly supports NHWC tensor format\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message:\n\u001b[1;32m   1393\u001b[0m   message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA possible workaround: Try disabling Grappler optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1394\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mby modifying the config for creating the session eg.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1395\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msession_config.graph_options.rewrite_options.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1396\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_meta_optimizer = True\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1397\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(node_def, op, message)\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node 'NLP_in_Action/Read/ReadVariableOp' defined at (most recent call last):\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n      app.start()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_4370/3755568974.py\", line 7, in <module>\n      create_projection(projection_data, projection_name)\n    File \"/tmp/ipykernel_4370/1444917779.py\", line 28, in create_projection\n      embedding = tf.Variable(projection_matrix,\nNode: 'NLP_in_Action/Read/ReadVariableOp'\nCould not find variable NLP_in_Action. This could mean that the variable has been deleted. In TF1, it can also mean the variable is uninitialized. Debug info: container=localhost, status error message=Container localhost does not exist. (Could not find resource: localhost/NLP_in_Action)\n\t [[{{node NLP_in_Action/Read/ReadVariableOp}}]]\n\nOriginal stack trace for 'NLP_in_Action/Read/ReadVariableOp':\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"/opt/conda/lib/python3.10/site-packages/traitlets/config/application.py\", line 982, in launch_instance\n    app.start()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/opt/conda/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n    self.asyncio_loop.run_forever()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n    self._run_once()\n  File \"/opt/conda/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n    handle._run()\n  File \"/opt/conda/lib/python3.10/asyncio/events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n    result = self._run_cell(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n    return runner(coro)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_4370/3755568974.py\", line 7, in <module>\n    create_projection(projection_data, projection_name)\n  File \"/tmp/ipykernel_4370/1444917779.py\", line 28, in create_projection\n    embedding = tf.Variable(projection_matrix,\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 271, in __call__\n    return cls._variable_v2_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 250, in _variable_v2_call\n    return previous_getter(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 243, in <lambda>\n    previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variable_scope.py\", line 2758, in default_variable_creator_v2\n    return resource_variable_ops.ResourceVariable(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    return fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/variables.py\", line 273, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1721, in __init__\n    self._init_from_args(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py\", line 1963, in _init_from_args\n    value = gen_resource_variable_ops.read_variable_op(handle, dtype)\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py\", line 539, in read_variable_op\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py\", line 795, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/opt/conda/lib/python3.10/site-packages/tensorflow/python/framework/ops.py\", line 3798, in _create_op_internal\n    ret = Operation(\n"
     ]
    }
   ],
   "source": [
    "projection_name = \"NLP_in_Action\"\n",
    "projection_data = [\n",
    "    ('car', [0.34, -0.72]),\n",
    "    ('toy', [0.46, 0.39])\n",
    "]\n",
    "\n",
    "create_projection(projection_data, projection_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f0c74900ca11cd9a4678e4c10a950358c13d37493068129285070960ce9781b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
