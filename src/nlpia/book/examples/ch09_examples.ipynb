{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43e5b253-49da-49fa-bdfe-84f54928dfac",
   "metadata": {},
   "source": [
    "# 9. Improving retention with long short-term memory networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f8a823-b9de-4b26-913e-4a68468bd81e",
   "metadata": {},
   "source": [
    "## 9.1. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18ed9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 10:26:27.581805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 10:26:28.297698: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 10:26:28.297749: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-12 10:26:31.034169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 10:26:31.034425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-12 10:26:31.034440: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 10:26:34.149046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-12 10:26:34.150405: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-12 10:26:34.150844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fe374cb43607): /proc/driver/nvidia/version does not exist\n",
      "2022-12-12 10:26:34.153598: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 400, 50)           70200     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400, 50)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 20000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 20001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a278ac-6ee9-489d-8936-4cf6a7c9d830",
   "metadata": {},
   "source": [
    "### 9.1.1 Backpropogration through time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1d475b-df67-4ae1-89af-093e6542a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../../bigdata/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9870e479-604f-4bf6-b5f2-43a0c670394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_data(filepath):\n",
    "    \"\"\"\n",
    "    This is dependent on your training data source but we will try to generalize it as best as possible.\n",
    "    \"\"\"\n",
    "    positive_path = os.path.join(filepath, 'pos')\n",
    "    negative_path = os.path.join(filepath, 'neg')\n",
    "\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
    "        with open(filename, 'r') as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "\n",
    "    shuffle(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8667254-53ba-44a2-b859-5c54f67ccae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "\n",
    "            except KeyError:\n",
    "                pass  # No matching token in the Google w2v vocab\n",
    "\n",
    "        vectorized_data.append(sample_vecs)\n",
    "\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4123a82c-91f3-43da-ba4c-3ae161d85765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    \"\"\" Peel of the target values from the dataset \"\"\"\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7ace27-4de5-439e-bfd5-56d547a1d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "## http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "dataset = pre_process_data('../../bigdata/aclImdb/train/')\n",
    "\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "split_point = int(len(vectorized_data) * .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83618dcc-3e67-4f74-8702-ca2e634c26e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32         # How many samples to show the net before backpropogating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "# filters = 250           # Number of filters we will train\n",
    "# kernel_size = 3         # The width of the filters, actual filters will each be a matrix of weights of size: embedding_dims x kernel_size or 50 x 3 in our case\n",
    "# hidden_dims = 250       # Number of neurons in the plain feed forward net at the end of the chain\n",
    "epochs = 2              # Number of times we will pass the entire training dataset through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "088bc399-7c45-4e38-bc72-7fb6c2a44a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    \"\"\" For a given dataset pad with zero vectors or truncate to maxlen \"\"\"\n",
    "    new_data = []\n",
    "\n",
    "    # Create a vector of 0's the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "\n",
    "    for sample in data:\n",
    "\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af20eeb3-f982-40cb-ae72-a36ec31d45c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715ff5a2-b510-4da1-852b-42ea5ceff7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce test/train data by factor of 8\n",
      "Before changes: 20000 and 5000\n",
      "After reduction: 2500 and 625\n",
      "After pad_trunc: 2500 and 625\n",
      "400 300\n"
     ]
    }
   ],
   "source": [
    "## need to artificially reduce the size of test/train data for it to run on my machine\n",
    "## then do the pad_trunc as documented\n",
    "\n",
    "reducer = 8\n",
    "print(f\"Reduce test/train data by factor of {reducer}\")\n",
    "\n",
    "print(f\"Before changes: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = x_train[:int(len(x_train) / reducer)]\n",
    "x_test = x_test[:int(len(x_test) / reducer)]\n",
    "\n",
    "y_train = y_train[:int(len(y_train) / reducer)]\n",
    "y_test = y_test[:int(len(y_test) / reducer)]\n",
    "\n",
    "print(f\"After reduction: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "print(f\"After pad_trunc: {len(x_train)} and {len(x_test)}\")\n",
    "print(maxlen, embedding_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5503980-3662-4131-adca-6a10da01ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7e9f8cb-6412-4546-bca6-086ee9b488ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5368fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 400, 50)           70200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 400, 50)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 20000)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 20001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9517aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 10:50:25.161830: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "79/79 [==============================] - 64s 594ms/step - loss: 0.6306 - accuracy: 0.6492 - val_loss: 0.4786 - val_accuracy: 0.7712\n",
      "Epoch 2/2\n",
      "79/79 [==============================] - 32s 407ms/step - loss: 0.4948 - accuracy: 0.7724 - val_loss: 0.4391 - val_accuracy: 0.8000\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights1.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0953c3",
   "metadata": {},
   "source": [
    "### 9.1.2 Where does the rubber hit the road?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfd912de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"lstm_model1.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "model = model_from_json(json_string)\n",
    "\n",
    "model.load_weights('lstm_weights1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "725187d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 861ms/step\n",
      "Sample's sentiment, 1 - pos, 2 - neg : [0]\n",
      "Raw output of sigmoid function: [[0.39549184]]\n"
     ]
    }
   ],
   "source": [
    "sample_1 = \"I hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\"\n",
    "\n",
    "# We pass a dummy value in the first element of the tuple just because our helper expects it from the way processed the initial data.  That value won't ever see the network, so it can be whatever.\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "\n",
    "# Tokenize returns a list of the data (length 1 here)\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "\n",
    "# print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict_classes(test_vec)))\n",
    "# print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))\n",
    "## AttributeError: 'Sequential' object has no attribute 'predict_classes'\n",
    "\n",
    "predict_x = model.predict(test_vec) \n",
    "classes_x = np.argmax(predict_x,axis=1)\n",
    "\n",
    "print(\"Sample's sentiment, 1 - pos, 0 - neg : {}\".format(classes_x))\n",
    "print(\"Raw output of sigmoid function: {}\".format(predict_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa91a2d",
   "metadata": {},
   "source": [
    "### 9.1. Dirty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "226ebbfa-4d42-4a9a-927e-e87ff7ab3d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvzf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec81f413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded: 22560\n",
      "Equal: 12\n",
      "Truncated: 2428\n",
      "Avg length: 202.43216\n"
     ]
    }
   ],
   "source": [
    "def test_len(data, maxlen):\n",
    "    total_len = truncated = exact = padded = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample)\n",
    "        if len(sample) > maxlen:\n",
    "            truncated += 1\n",
    "        elif len(sample) < maxlen:\n",
    "            padded += 1\n",
    "        else:\n",
    "            exact +=1 \n",
    "    print('Padded: {}'.format(padded))\n",
    "    print('Equal: {}'.format(exact))\n",
    "    print('Truncated: {}'.format(truncated))\n",
    "    print('Avg length: {}'.format(total_len/len(data)))\n",
    "\n",
    "# dataset = pre_process_data('./aclImdb_v1/train')\n",
    "dataset = pre_process_data('../../bigdata/aclImdb/train/')\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "test_len(vectorized_data, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14092dee-0de2-4555-9486-a2ee641f35f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "\n",
    "maxlen = 200\n",
    "batch_size = 32         # How many samples to show the net before backpropagating the error and updating the weights\n",
    "embedding_dims = 300    # Length of the token vectors we will create for passing into the Convnet\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# dataset = pre_process_data('./aclImdb_v1/train')\n",
    "dataset = pre_process_data('../../bigdata/aclImdb/train/')\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "\n",
    "split_point = int(len(vectorized_data)*.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16779c70-d38d-48c8-a5bd-4322ab114421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce test/train data by factor of 8\n",
      "Before changes: 20000 and 5000\n",
      "After reduction: 2500 and 625\n",
      "After pad_trunc: 2500 and 625\n",
      "200 300\n",
      "Build model...\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_3 (LSTM)               (None, 200, 50)           70200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 200, 50)           0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 10000)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 10001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,201\n",
      "Trainable params: 80,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]\n",
    "\n",
    "## need to artificially reduce the size of test/train data for it to run on my machine\n",
    "## then do the pad_trunc as documented\n",
    "\n",
    "reducer = 8\n",
    "print(f\"Reduce test/train data by factor of {reducer}\")\n",
    "\n",
    "print(f\"Before changes: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = x_train[:int(len(x_train) / reducer)]\n",
    "x_test = x_test[:int(len(x_test) / reducer)]\n",
    "\n",
    "y_train = y_train[:int(len(y_train) / reducer)]\n",
    "y_test = y_test[:int(len(y_test) / reducer)]\n",
    "\n",
    "print(f\"After reduction: {len(x_train)} and {len(x_test)}\")\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "print(f\"After pad_trunc: {len(x_train)} and {len(x_test)}\")\n",
    "print(maxlen, embedding_dims)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "num_neurons = 50\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a36d98bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 14:47:11.622780: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 600000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "79/79 [==============================] - 43s 394ms/step - loss: 0.6436 - accuracy: 0.6080 - val_loss: 0.5483 - val_accuracy: 0.7440\n",
      "Epoch 2/2\n",
      "79/79 [==============================] - 19s 245ms/step - loss: 0.5106 - accuracy: 0.7616 - val_loss: 0.9454 - val_accuracy: 0.5920\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model7.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"lstm_weights7.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364b0a18",
   "metadata": {},
   "source": [
    "### 9.1.5 Words are hard. Letters are easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db695f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = pre_process_data('./aclImdb_v1/train')\n",
    "# dataset = pre_process_data('../../bigdata/aclImdb_v1/train/')\n",
    "# expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb03a91a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1325.06964\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def avg_len(data):\n",
    "    total_len = 0\n",
    "    for sample in data:\n",
    "        total_len += len(sample[1])\n",
    "    print(total_len/len(data))\n",
    "\n",
    "print(avg_len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "189e5c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_data(data):\n",
    "    \"\"\" Shift to lower case, replace unknowns with UNK, and listify \"\"\"\n",
    "    new_data = []\n",
    "    VALID = 'abcdefghijklmnopqrstuvwxyz123456789\"\\'?!.,:; '\n",
    "    for sample in data:\n",
    "        new_sample = []\n",
    "        for char in sample[1].lower():  # Just grab the string, not the label\n",
    "            if char in VALID:\n",
    "                new_sample.append(char)\n",
    "            else:\n",
    "                new_sample.append('UNK')\n",
    "       \n",
    "        new_data.append(new_sample)\n",
    "    return new_data\n",
    "\n",
    "listified_data = clean_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1296fd03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def char_pad_trunc(data, maxlen):\n",
    "    \"\"\" We truncate to maxlen or add in PAD tokens \"\"\"\n",
    "    new_dataset = []\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            new_data = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            pads = maxlen - len(sample)\n",
    "            new_data = sample + ['PAD'] * pads\n",
    "        else:\n",
    "            new_data = sample\n",
    "        new_dataset.append(new_data)\n",
    "    return new_dataset\n",
    "\n",
    "maxlen = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf20895c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dicts(data):\n",
    "    \"\"\" Modified from Keras LSTM example\"\"\"\n",
    "    chars = set()\n",
    "    for sample in data:\n",
    "        chars.update(set(sample))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "    return char_indices, indices_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a70d6018",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_encode(dataset, char_indices, maxlen):\n",
    "    \"\"\" \n",
    "    One hot encode the tokens\n",
    "    \n",
    "    Args:\n",
    "        dataset  list of lists of tokens\n",
    "        char_indices  dictionary of {key=character, value=index to use encoding vector}\n",
    "        maxlen  int  Length of each sample\n",
    "    Return:\n",
    "        np array of shape (samples, tokens, encoding length)\n",
    "    \"\"\"\n",
    "    X = np.zeros((len(dataset), maxlen, len(char_indices.keys())))\n",
    "    for i, sentence in enumerate(dataset):\n",
    "        for t, char in enumerate(sentence):\n",
    "            X[i, t, char_indices[char]] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40241b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset = pre_process_data('./aclImdb_v1/train')\n",
    "dataset = pre_process_data('../../bigdata/aclImdb_v1/train/')\n",
    "expected = collect_expected(dataset)\n",
    "listified_data = clean_data(dataset)\n",
    "\n",
    "maxlen = 1500\n",
    "common_length_data = char_pad_trunc(listified_data, maxlen)\n",
    "\n",
    "char_indices, indices_char = create_dicts(common_length_data)\n",
    "encoded_data = onehot_encode(common_length_data, char_indices, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8396b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_point = int(len(encoded_data)*.8)\n",
    "\n",
    "x_train = encoded_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = encoded_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8272655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_5 (LSTM)               (None, 1500, 40)          6560      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1500, 40)          0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 60000)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 60001     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,561\n",
      "Trainable params: 66,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten, LSTM\n",
    "\n",
    "\n",
    "num_neurons = 40\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, len(char_indices.keys()))))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'])\n",
    "model.compile('rmsprop', 'binary_crossentropy',  metrics=['accuracy'], run_eagerly=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a449dd78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model_structure \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto_json()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_lstm_model3.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1662\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1660\u001b[0m logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1662\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected result of `train_function` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1664\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empty logs). Please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1665\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minformation of where went wrong, or file a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1668\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124missue/bug to `tf.keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1669\u001b[0m     )\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))\n",
    "model_structure = model.to_json()\n",
    "with open(\"char_lstm_model3.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "model.save_weights(\"char_lstm_weights3.h5\")\n",
    "print('Model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba42ad",
   "metadata": {},
   "source": [
    "### 9.1.7 My turn to speak more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82a6e776-a65f-4048-96d0-d4b6fd9e24a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23288ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "455adb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 375542\n",
      "total chars: 50\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for txt in gutenberg.fileids():\n",
    "    if 'shakespeare' in txt:\n",
    "        text += gutenberg.raw(txt).lower()\n",
    "\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "414c3133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eafa173b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 125168\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5bb41ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1904/3464639879.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
      "/tmp/ipykernel_1904/3464639879.py:3: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7677bb20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 128)               91648     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                6450      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 50)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/rmsprop.py:143: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d59606c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "978/978 [==============================] - 93s 91ms/step - loss: 2.0414\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 92s 94ms/step - loss: 1.6827\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 99s 101ms/step - loss: 1.5783\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 91s 93ms/step - loss: 1.5172\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 79s 81ms/step - loss: 1.4740\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 83s 85ms/step - loss: 1.4429\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 80s 82ms/step - loss: 1.4172\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 101s 104ms/step - loss: 1.3980\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 105s 107ms/step - loss: 1.3795\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 97s 99ms/step - loss: 1.3641\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 89s 91ms/step - loss: 1.3532\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 90s 92ms/step - loss: 1.3428\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 89s 91ms/step - loss: 1.3333\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 86s 88ms/step - loss: 1.3245\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 94s 96ms/step - loss: 1.3189\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 85s 87ms/step - loss: 1.3087\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 92s 95ms/step - loss: 1.3047\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 92s 94ms/step - loss: 1.2990\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 91s 93ms/step - loss: 1.2942\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 88s 90ms/step - loss: 1.2871\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 82s 84ms/step - loss: 1.2831\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 79s 81ms/step - loss: 1.2785\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 79s 80ms/step - loss: 1.2749\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 83s 84ms/step - loss: 1.2690\n",
      "Model saved.\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 73s 75ms/step - loss: 1.2657\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 86s 87ms/step - loss: 1.2611\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 79s 81ms/step - loss: 1.2583\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 78s 80ms/step - loss: 1.2554\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 80s 82ms/step - loss: 1.2504\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 79s 80ms/step - loss: 1.2466\n",
      "Model saved.\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "batch_size = 128\n",
    "\n",
    "model_structure = model.to_json()\n",
    "with open(\"shakes_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "for i in range(5):\n",
    "    model.fit(X, y,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs)\n",
    "\n",
    "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1))\n",
    "    print('Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "94e65013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### NOT IN CHAPTER, Just to reproduce output\n",
    "\n",
    "from keras.models import model_from_json\n",
    "\n",
    "with open('shakes_lstm_model.json', 'r') as f:\n",
    "    model_json = f.read()\n",
    "    \n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('shakes_lstm_weights_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0fec08f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8461b1cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"he.\n",
      "(o heauen! a beast that wants discou\"\n",
      "he.\n",
      "(o heauen! a beast that wants discoue thee:\n",
      "but what is the stand of the straight,\n",
      "that may be made me to the second byrnanes\n",
      "\n",
      "   cassi. i haue seene thee thinke the tran to the strikes,\n",
      "and the straine, and the connerous to the secrees\n",
      "\n",
      "   cassi. shall we must you this point, and the state\n",
      "in the chan and be a right and so much vnrtyes the most conity,\n",
      "the chances of the straight, and the secree\n",
      "\n",
      "   cassi. so the secret and the rea\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"he.\n",
      "(o heauen! a beast that wants discou\"\n",
      "he.\n",
      "(o heauen! a beast that wants discouer, state\n",
      "the world be receiue a constanc'd display,\n",
      "who haue i you shall he is a man charge,\n",
      "i should welo not be a cassius, when you know not\n",
      "\n",
      "   ross. thou louicus you do you are you,\n",
      "and with the straiges and haue made me forme,\n",
      "i loue me the second king, which it selfe in fire the side,\n",
      "become my sight, and all the selues beares and men\n",
      "\n",
      "   cask. it like the capition finde of the resse,\n",
      "and f\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"he.\n",
      "(o heauen! a beast that wants discou\"\n",
      "he.\n",
      "(o heauen! a beast that wants discouey thee:\n",
      "anshed we dreaching s'dore hold him a clowne gooe tripes,\n",
      "perfer the else haue seeght and my dile,\n",
      "and you, that will world sweaty of vs, ha's th'\n",
      "lords it, you shall powerd all prin,\n",
      "what he was like a more manions is you,\n",
      "enchestrona, but without one artioes and gone\n",
      "and thinke you, and eyes flye, riues,\n",
      "let him about day in putching preaser leaoth\n",
      "to some of your a\n",
      "morny on the sidetin\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "for diversity in [0.2, 0.5, 1.0]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d08c2d",
   "metadata": {},
   "source": [
    "### 9.1.9 Other kinds of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bfdfcf16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import GRU\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(num_neurons, return_sequences=True, input_shape=X[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930c2fb",
   "metadata": {},
   "source": [
    "### 9.1.10 Going deeper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a43ba23-dc1a-433f-b022-5f9b7205f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons_2 = num_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "446f686a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, return_sequences=True, input_shape=X[0].shape))\n",
    "model.add(LSTM(num_neurons_2, return_sequences=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
